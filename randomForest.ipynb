{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import skew, kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(feature):\n",
    "        return np.hstack([\n",
    "            np.mean(feature), np.var(feature), np.std(feature),\n",
    "            np.median(feature), np.max(feature), \n",
    "            np.min(feature), skew(feature), \n",
    "            kurtosis(feature),\n",
    "        ])\n",
    "\n",
    "def extract_features(audio_file):\n",
    "\n",
    "        # Load the audio file using librosa\n",
    "        y, sr = librosa.load(audio_file, sr=None, mono=True)\n",
    "\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)\n",
    "        #alla 20 pÃ¥ mfcc\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "        \n",
    "        chroma_stats = []\n",
    "        for i in range(chroma_stft.shape[0]):\n",
    "            chroma_stats.append(calc_stats(chroma_stft[i, :]))  \n",
    "        chroma_stats = np.hstack(chroma_stats)\n",
    "        #print(f\"Chroma STFT Shape: {chroma_stft.shape}\")\n",
    "        #print(f\"Chroma STFT stats Shape: {chroma_stats.shape}\")\n",
    "\n",
    "        spectral_centroid_stats = calc_stats(np.hstack(spectral_centroid))\n",
    "        #print(f\"Spectral Centroid  Shape: {spectral_centroid.shape}\")\n",
    "        #print(f\"Spectral Centroid stats  Shape: {spectral_centroid_stats.shape}\")\n",
    "\n",
    "\n",
    "        spectral_contrast_stats = []\n",
    "        for i in range(spectral_contrast.shape[0]):\n",
    "            spectral_contrast_stats.append(calc_stats(spectral_contrast[i, :])) \n",
    "        spectral_contrast_stats = np.hstack(spectral_contrast_stats)\n",
    "        #print(f\"Spectral Contrast Shape: {spectral_contrast.shape}\")\n",
    "        #print(f\"Spectral Contrast stats Shape: {spectral_contrast_stats.shape}\")\n",
    "\n",
    "        zero_crossing_stats = calc_stats(np.hstack(zero_crossing_rate))\n",
    "        #print(f\"Zero Crossing Rate Shape: {zero_crossing_rate.shape}\")\n",
    "        #print(f\"Zero Crossing Rate stast Shape: {zero_crossing_stats.shape}\")\n",
    "\n",
    "\n",
    "        mfcc_stats = []\n",
    "        for i in range(mfccs.shape[0]):\n",
    "            mfcc_stats.append(calc_stats(mfccs[i, :]))  \n",
    "\n",
    "        mfcc_stats = np.hstack(mfcc_stats)\n",
    "        #print(f\"MFCC Stats Shape: {mfccs.shape}\")\n",
    "        #print(f\"MFCC Stats stats Shape: {mfcc_stats.shape}\")\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # Aggregate each feature into mean and variance\n",
    "        features =np.hstack( [\n",
    "            chroma_stats,\n",
    "            spectral_centroid_stats,\n",
    "            spectral_contrast_stats,\n",
    "            zero_crossing_stats,\n",
    "            mfcc_stats])\n",
    "\n",
    "      \n",
    "        #print(f\"Final Features Shape: {features.shape}\")\n",
    "        return np.array(features)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code from fma https://github.com/mdeff/fma/blob/master/utils.py#L183\n",
    "def load_tracks(filepath):\n",
    "    if 'tracks' in filepath:\n",
    "        # Load tracks.csv\n",
    "        tracks = pd.read_csv(filepath, index_col=0, header=[0, 1])\n",
    "\n",
    "        \n",
    "        COLUMNS = [('track', 'tags'), ('album', 'tags'), ('artist', 'tags'),\n",
    "                   ('track', 'genres'), ('track', 'genres_all')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].map(ast.literal_eval)\n",
    "\n",
    "        COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),\n",
    "                   ('album', 'date_created'), ('album', 'date_released'),\n",
    "                   ('artist', 'date_created'), ('artist', 'active_year_begin'),\n",
    "                   ('artist', 'active_year_end')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = pd.to_datetime(tracks[column])\n",
    "\n",
    "       \n",
    "        SUBSETS = ('small', 'medium', 'large')\n",
    "        try:\n",
    "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
    "                'category', categories=SUBSETS, ordered=True)\n",
    "        except (ValueError, TypeError):\n",
    "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
    "                pd.CategoricalDtype(categories=SUBSETS, ordered=True))\n",
    "\n",
    "        COLUMNS = [('track', 'genre_top'), ('track', 'license'),\n",
    "                   ('album', 'type'), ('album', 'information'),\n",
    "                   ('artist', 'bio')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].astype('category')\n",
    "\n",
    "        return tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 52)\n"
     ]
    }
   ],
   "source": [
    "audio_dir='fma_small'\n",
    "\n",
    "feature_extraction_test = extract_features('fma_small/000/000002.mp3')\n",
    "\n",
    "filepath = 'fma_small/fma_metadata/tracks.csv'\n",
    "tracks = load_tracks(filepath)\n",
    "fma_small = tracks[tracks['set', 'subset'] == 'small']\n",
    "\n",
    "# Print the shape of the small subset\n",
    "print(fma_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 52)\n"
     ]
    }
   ],
   "source": [
    "audio_dir='fma_small'\n",
    "\n",
    "feature_extraction_test = extract_features('fma_small/000/000002.mp3')\n",
    "\n",
    "filepath = 'fma_small/fma_metadata/tracks.csv'\n",
    "tracks = load_tracks(filepath)\n",
    "fma_small = tracks[tracks['set', 'subset'] == 'small']\n",
    "\n",
    "# Print the shape of the small subset\n",
    "print(fma_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5802586078643799\n",
      "0.5830786228179932\n",
      "0.6143083572387695\n",
      "0.5498011112213135\n",
      "0.5498833656311035\n",
      "0.5855553150177002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(audio_dir, directory, filename)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m     20\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(genre_label)\n",
      "Cell \u001b[1;32mIn[38], line 15\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(audio_file)\u001b[0m\n\u001b[0;32m     12\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_file, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mono\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m chroma_stft \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mchroma_stft(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m---> 15\u001b[0m spectral_centroid \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectral_centroid\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m spectral_contrast \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     17\u001b[0m zero_crossing_rate \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mzero_crossing_rate(y\u001b[38;5;241m=\u001b[39my)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Desktop\\MusicInformatics\\.venv\\Lib\\site-packages\\librosa\\feature\\spectral.py:156\u001b[0m, in \u001b[0;36mspectral_centroid\u001b[1;34m(y, sr, S, n_fft, hop_length, freq, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the spectral centroid.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03mEach frame of a magnitude spectrogram is normalized and treated as a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m>>> ax.set(title='log Power spectrogram')\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# input is time domain:y or spectrogram:s\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m \u001b[43m_spectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misrealobj(S):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpectral centroid is only defined \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith real-valued input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Desktop\\MusicInformatics\\.venv\\Lib\\site-packages\\librosa\\core\\spectrum.py:2945\u001b[0m, in \u001b[0;36m_spectrogram\u001b[1;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m   2941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2942\u001b[0m         )\n\u001b[0;32m   2943\u001b[0m     S \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2944\u001b[0m         np\u001b[38;5;241m.\u001b[39mabs(\n\u001b[1;32m-> 2945\u001b[0m             \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2946\u001b[0m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2947\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2948\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2949\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2950\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2951\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2952\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2953\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2954\u001b[0m         )\n\u001b[0;32m   2955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m power\n\u001b[0;32m   2956\u001b[0m     )\n\u001b[0;32m   2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m S, n_fft\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Desktop\\MusicInformatics\\.venv\\Lib\\site-packages\\librosa\\core\\spectrum.py:387\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[0;32m    385\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 387\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m \u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Desktop\\MusicInformatics\\.venv\\Lib\\site-packages\\numpy\\fft\\_pocketfft.py:414\u001b[0m, in \u001b[0;36mrfft\u001b[1;34m(a, n, axis, norm, out)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m--> 414\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Desktop\\MusicInformatics\\.venv\\Lib\\site-packages\\numpy\\fft\\_pocketfft.py:94\u001b[0m, in \u001b[0;36m_raw_fft\u001b[1;34m(a, n, axis, is_real, is_forward, norm, out)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ((shape \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m!=\u001b[39m a\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mor\u001b[39;00m shape[axis] \u001b[38;5;241m!=\u001b[39m n_out)):\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput array has wrong shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Extract features for each track in fma_small\n",
    "for idx, row in fma_small.iterrows():\n",
    "    #start_time = time.time()\n",
    "\n",
    "    track_id = row.name \n",
    "    genre_label = row[('track', 'genre_top')]  \n",
    "    \n",
    "    # Construct the file path\n",
    "    directory = '{:03d}'.format(track_id // 1000)\n",
    "    filename = '{:06d}.mp3'.format(track_id)\n",
    "    file_path = os.path.join(audio_dir, directory, filename)\n",
    "    \n",
    "    try:\n",
    "        features = extract_features(file_path)\n",
    "        X.append(features)\n",
    "        y.append(genre_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "    if idx % 1000 == 0 and idx != 0:\n",
    "            print(f\"Processed {idx} files so far\")\n",
    "    #elapsed_time = time.time() - start_time\n",
    "    #print(elapsed_time)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "np.save('X.npy', X)\n",
    "np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X and y from the saved .npy files\n",
    "\n",
    "XX = np.load('X.npy', allow_pickle=True)\n",
    "yy = np.load('y.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6395, 15), y_train shape: (6395,)\n",
      "X_test shape: (1599, 15), y_test shape: (1599,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200,n_jobs=-1)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print that training is done\n",
    "print(\"Training complete!\")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(f\"Cross-validation accuracy: {scores.mean():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\Desktop\\MusicInformatics\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "#Best parameters found: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy: 0.4586\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.72%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Electronic       0.51      0.52      0.52       200\n",
      " Experimental       0.44      0.30      0.36       200\n",
      "         Folk       0.48      0.59      0.53       200\n",
      "      Hip-Hop       0.51      0.61      0.55       199\n",
      " Instrumental       0.50      0.51      0.50       200\n",
      "International       0.52      0.46      0.49       200\n",
      "          Pop       0.25      0.20      0.22       200\n",
      "         Rock       0.47      0.55      0.50       200\n",
      "\n",
      "     accuracy                           0.47      1599\n",
      "    macro avg       0.46      0.47      0.46      1599\n",
      " weighted avg       0.46      0.47      0.46      1599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new audio file you want to predict\n",
    "new_audio_file = 'path_to_new_audio_file.mp3'\n",
    "\n",
    "# Extract features from the new audio file\n",
    "new_audio_features = extract_features(new_audio_file)\n",
    "\n",
    "# Reshape the features to match the input shape expected by the model (1 sample with n features)\n",
    "new_audio_features = new_audio_features.reshape(1, -1)  # Reshape to (1, n_features)\n",
    "# Predict the genre for the new audio file\n",
    "predicted_genre = clf.predict(new_audio_features)\n",
    "\n",
    "# Print the predicted genre\n",
    "print(f\"The predicted genre is: {predicted_genre[0]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
